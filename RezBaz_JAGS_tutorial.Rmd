---
title: "Introduction to Bayesian Statistics in R"
author: "Kelly Heilman"
date: "5/12/2020"
output: html_document
---

### Overview 
In this workshop, we will cover the very basics of Bayesian Statistics, and go over an example of how to specify your first bayesian linear regression in R using JAGS!

### Prerequisites:
If you want to follow along with the code you need to:
- install JAGS itself http://mcmc-jags.sourceforge.net/
- have R/Rstudio installed
- have the "rjags", "coda", and "ggplot2" libraries installed 
- download our dataset at:


### Objectives
We will cover the very basics of Bayesian Statistics, and go over an example of how to specify your first bayesian linear regression in R!

We will go over some further resources and things to watch out for when working in the bayes frame of mind. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rjags)
library(coda)
library(ggplot2)

```

### What are the advantages of Bayesian analysis?
  - Treats all parameters as random variables, including our data!
      - rather than assuming our data and covariates are the single *true* estimate, we acknowlege that these values come from a distribution that represents the True (with a capital T) value of the process we are interested in.
  - Estimation and parsing uncertainty
      - bayesian predictive intervals can include different sources of uncertainty: unexplained process error, parameter uncertainty, data uncertainty, and driver uncertainty
      
  - lends itself to heiarchical and different process models 
      - borrow strength across different groups or treatments
      - can specify appropriate distributions, and different process models, including state-space models, logistic functions, etc. 


### Bayes Theorem
Bayes theorem is the basis for bayesian analyses. It states that the conditional probability of A, given B is given by the Probabiliity of B given A, multiplied by Probability of A, and normalized by the Probability of B

$P(A|B) = {P(B|A)*P(A)}/{P(B)}$

Likelihood: P(B|A) 
Prior probabilty of A
Marginalize by the Probability of B

So what does this mean in the context of a model?

Lets say we have a regression model with parameters denoted by $\theta$ and data denoted by $X$ we can use bayes theorem to determine the conditional probability of our model parameters, given the data $P(\theta|X)$, also called the *posterior*. 

The posterior is equal to the 





We won't discuss this section too much, as I want to focus on implemetation in R

### Multiple ways to run Bayesian models in R
  a.	JAGS
  b.	BUGS/WINBUGS
  c.	STAN
  d.	NIMBLE

### A simple Linear regression example
We will start off thinking about a simple linear regression approach, where we are estimating the slope and intercept of the relationship between two variables in a sample relationship. You might use `lm()` to fit this model using maximum likelihood, frequentist approach. We will use the cars dataset loaded into R.

Frequetist approaches based on maximum likelhood methods.


```{r }
# I am a big GGPLOT fan, so sorry base R plotters...
ggplot(cars, aes(dist, speed))+geom_point()+theme_bw(base_size = 12) + ylab("Distance")+xlab("Speed")


# do a basic linear regression using "lm"

speed.dist.lm <- lm(speed ~ dist, data = cars)
lm.results <- summary(speed.dist.lm)

summary(speed.dist.lm)

ggplot(cars, aes(dist, speed))+geom_point()+theme_bw(base_size = 12) + ylab("Distance")+xlab("Speed")+geom_abline(intercept = lm.results$coefficients[1,1], slope = lm.results$coefficients[2,1], color = "red")

```

### A simple linear regression in JAGS

I mentioned that in Bayesian models, all variables are treated as random, that means that all variables are/can be estimated within the model. In our linear regression, this includes our dependant variable, y. 

Why might this be useful?
  - Well y is a measurement taken by me, an imperfect human being. 
  - We typically can't directly measure the "TRUE" process or value that we are interested in. 


##### The Linear Regression Model

#### Data Model
$$y_{i}~\sim normal(\mu, \sigma^2)$$

#### Process Model 
We specify the linear regression withe slope $\beta$, intercept $\alpha$ and our independent variable $x$

$$mu = \beta*x + \alpha$$

While we are specifying a very simple model, the process model could be any model that you can write out (logistic growth, muliple linear regression, state-space model etc)


### Priors:
In Bayes models you specify distributions that you want to sample the parameters from. We wont go into detail on how to select priors in this short workshop, but keep in mind that prior selection is quite important. The benefit of Bayesian modelling is that you can choose "uninformative priors" or "informative priors". 

#### Uninformative priors:

Specify a very broad distribution over which to sample for our parameter, and basically state that we have no prior knowledge of what value our parameter should be, besides the distribution shape. 

#### Informative priors:

Specify that we have *some* prior knowledge of what value the parameter should be. 
Why might this be useful?

Well, alot of science does not exist in a vacuum, and we likely already have some prior knowledge of what we expect. For example, this might be useful if we have already done a study where we quantified the relationship between y and x, and we know that the value of beta should fall within XX-XX. 

This may not be appear to be super useful in a basic linear regression, it can also be useful in much more complex models, where multiple parameters could trade off. 

In this model we will specify uninformative priors for our prarameters, which assume that the true value of beta is sampled from a normal distribution with mean of 0 and variance of 1000. We also specify a prior for $\sigma^2$

$$\beta \sim normal(0, 0.001)$$
$$\alpha \sim normal(0, 0.001)$$
$$\sigma^2 \sim invgamma(0.01, 0.01$$

Note that variances must be non-negative, hence we cannot use a normal prior as a distribution

## JAGS (Just Another Gibbs Sampler)
We will use JAGS ('Just Another Gibbs Sampler') to run this particular bayesian model. JAGS is designed to run Bayesian models using Markoc Chain Monte Carlo simulations (MCMC). JAGS is just one option among many for bayesian analysis. We can call and run jags from R utilizing the 'rjags' library. Note that there are other R libraries that can help with this including 'R2jags'. 



## Defining our JAGS model
To specify a JAGS model, you must define all the random distributions

```{r}
asimple_linear_regression <- "model{

  # Likelihood
  for(i in 1:n){
    y[i]   ~ dnorm(mu[i],inv.var) # data model 
    mu[i] <- alpha + beta*xvals[i]  # process model for the linear regression
  }

 # Priors
  beta ~ dnorm(0,0.0001)
  alpha ~ dnorm(0,0.0001)

  
  inv.var   ~ dgamma(0.01, 0.01) # Prior for the inverse variance
  sigma     <- 1/sqrt(inv.var) # note that in JAGS the second argument of the normal distribution is 1/sigma^2, or the precision Tau

}"
```



### Formatting the data + inital values for each chain
Jags expects all data to in a list.
```{r}
car.data <- list(xvals = cars$dist, y = cars$speed, n = length(cars$speed))
```

### Initialize the model
Initializing the model using jags.model. Note that you can (and probably should generate intial values, but here we let jags do the default).
This step sets up the JAGS model, loads the data, and specifies all paramets and priors.
```{r}
jags.model   <- jags.model (file = textConnection(asimple_linear_regression),
                             data = car.data,
                             #inits = inits,
                             n.chains = 3)
```


### Running MCMC chains 
JAGS can run mulitple MCMC chains, ideally with random starting values. This is a good idea beacuse it ensures that our model "converges" on the same parameter space consistently. Many times people run at least 3 chains.

The funciton "coda.samples" runs the MCMC chains of our model for a specified number of MCMC iteraations (specified by 'n.iter'), and is set up to 'trace' or output all parameters/variables of interest specified by variable.names. Often models are run for a high number of iterations, so 5000 MCMC samples is quite small, but we will use it for this tutorial. 

```{r}
jags.reg.out   <- coda.samples (model = jags.model,
                            variable.names = c("alpha","beta", "inv.var"),
                                n.iter = 5000)

summary(jags.reg.out)
```

### Assessing Model output 
coda.samples gives us a `mcmc.list` 

```{r}
class(jags.reg.out)
summary(jags.reg.out)
traceplot(jags.reg.out)
```

### Have our chains converged?
We need to make sure that our random samples are converging to the same parameter space. Tools to do this include:
 -traceplots
 -GBR statistics (ideally < 1.01)
 -remove "burn-in"
 
 
```{r}
traceplot(jags.reg.out)
gelman.diag(jags.reg.out)
```

### Check for Autocorrelation
# Many people thin their chains by taking only the "ith" MCMC iteration 
```{r}
acfplot(jags.reg.out)
```
### Lets plot up our parameter estimates + uncertainty

# plot the credible intervals around the regression line, which are analogous to the uncertainty/error provided in a frequentist regression 
```{r}
head(jags.reg.out)

jags.mat <- as.matrix(jags.reg.out) 

xpred <- min(cars$dist): max(cars$dist)
plot(cars$dist, cars$speed)
cred.lines <- list()

# loop through to get the lines
for(i in 1000:2000){
 cred.lines[[i]]<-  data.frame(MCMC.step = i,
   xpreds = xpred, 
                              ypreds = jags.mat[i,"alpha"] + jags.mat[i,"beta"]*xpred)
}

cred.intervals <- do.call(rbind, cred.lines)


ggplot(cars, aes(dist, speed))+geom_point()+theme_bw(base_size = 12) + ylab("Distance")+xlab("Speed")+geom_line(data = cred.intervals, aes(x =xpreds, y = ypreds, group = MCMC.step), color = "grey", alpha = 0.5)


```

# Lets also simulate the credible intervals and predictive intervals from random pairs of samples of the mcmc output
```{r}

nsamp <- 2500
mcmcsamples <- sample(nrow(jags.mat),nsamp)
xpred <- min(cars$dist): max(cars$dist) 	
ypred <- matrix(0.0,nrow=nsamp,ncol=npred)	
ycred <- matrix(0.0,nrow=nsamp,ncol=npred)	
```

Next we'll set up a loop where we'll calculate the expected value of y at each x for each pair of regression parameters and then add additional random error from the data model. 

```{r}


for(i in 1:2500){
  params <-  jags.mat[mcmcsamples[i],]
  ycred[i,] <- params["alpha"] + params["beta"]*xpred # same as above
  ypred[i,] <- rnorm(n = npred, mean = ycred[i,], sd = 1/sqrt(params["inv.var"])) #draw from normal distribution with our estimated uncertainty!
}


#combine the  ci, pi, and xpreds into a dataframe ot plot in ggplot2

ci.and.pi <- data.frame(xvals = xpred, 
                       ci.med = apply(ycred, 2, quantile, 0.5),
                       ci.lo= apply(ycred, 2, quantile, 0.025),
                       ci.hi= apply(ycred, 2, quantile, 0.975),
                       pi.med= apply(ypred, 2, quantile, 0.5),
                       pi.lo= apply(ypred, 2, quantile, 0.025),
                       pi.hi=apply(ypred, 2, quantile, 0.975))  

# now make a pretty plot with all the ci and pis on it!

ggplot()+geom_point(data = cars, aes(dist, speed))+theme_bw(base_size = 12) +  ylab("Speed")+xlab("Distance")+geom_ribbon(data = ci.and.pi, aes(x = xvals, ymin = pi.lo, ymax = pi.hi), fill = "blue", alpha = 0.6)+geom_ribbon(data = ci.and.pi, aes(x = xvals, ymin = ci.lo, ymax = ci.hi), fill = "grey", alpha = 0.6)

```

#--------------------- Breakout group Discussion ------------------------------

In your groups, discuss:
1. What do the credible intervals represent? 
2. What aspects of uncertainty do you think the credible intervals represent?
3. what aspects of uncertainty do the bayesian predictive intervals represent?

#------------------------------------------------------------------------------

## Uncertainty:
1. parameter uncertainty (credible intervals, the 95% CI around $\beta$ and $\alpha$)
2. process uncertainty (sigma)
3. driver/data uncertainty (What if we had multiple measurements of our dependant variable?)


### To simulate some observations from the cars dataset
Jags expects all data to in a list.
```{r}
speeds1 <- speeds2 <- speeds3 <- matrix(NA, nrow = length(cars$speed), ncol = 1)
for(i in 1:length(cars$speed)){
speeds1 [i]<- rnorm(1, cars[i,]$speed, sd = 1)
speeds2 [i]<- rnorm(1, cars[i,]$speed, sd = 1)
speeds3 [i]<- rnorm(1, cars[i,]$speed, sd = 1)
}
cars.mult.measure <- data.frame(dist = rep(cars$dist, 3), 
                                speed = c(speeds1, speeds2, speeds3 ), 
                                observer = rep(1:3, each = length(cars$speed)))


car.obs.data <- list(xvals = cars.mult.measure$dist, y = cars.mult.measure$speed, obs = cars.mult.measure$observer, n = length(cars.mult.measure$speed))
```



#----------------------------Including Data Uncertainty-------------------------
Lets say that our variable "Speed" was measured by taking 3 different radar measurements, each slighly different. How do we know what the "true" speed is?

Open question for the group: How would you typically deal with this?


What if we could include this uncertainty in our model, in a way that propogates forward to the estimates of distance?

We can!

One way to do this is to add a data model for speed:
Lets say we took multiple measurements of speed for several distances, and determined that our speed estimates have a SD = 2.5. 

We can use this measurement to estimate the "true speed" in our model:
$$truespeed_{i} \sim Normal(mean = speed_{i}, precison = tau_{speed})$$
Since we already have some prior knowledge of what the precision on our speed estimates should be, we can include an informative prior:

$$tau_{speed} \sim gamma(10, 0.5)$$

```{r}
a_linear_regression_driver_unc <- "model{

  # Likelihood
  for(i in 1:n){
    
    
   true_speed[i] ~ dnorm(xvals[i], tau_speed) # data model for speed measurements
    
    y[i]   ~ dnorm(mu[i],inv.var) # data model for distance (y variable)
    mu[i] <- alpha + beta*true_speed[i]  # process model for the linear regression
  }

 # Priors
  beta ~ dnorm(0,0.0001)
  alpha ~ dnorm(0,0.0001)

  
  inv.var   ~ dgamma(0.01, 0.01) # Prior for the inverse variance
  sigma     <- 1/sqrt(inv.var) # note that in JAGS the second argument of the normal distribution is 1/sigma^2, or the precision Tau
  
  # prior for speed precsion
  tau_speed   ~ dgamma(10, 0.5) 

}"
```


```{r}
car.data <- list(xvals = cars$dist, y = cars$speed, n = length(cars$speed))
```

### Initialize the model
Initializing the model using jags.model. Note that you can (and probably should generate intial values, but here we let jags do the default).
This step sets up the JAGS model, loads the data, and specifies all paramets and priors.
```{r}
jags.model.drive   <- jags.model (file = textConnection(a_linear_regression_driver_unc ),
                             data = car.data,
                             #inits = inits,
                             n.chains = 3)
jags.drive.unc.out   <- coda.samples (model = jags.model.drive,
                            variable.names = c("alpha","beta", "inv.var", "tau_speed" ),
                                n.iter = 5000)

summary(jags.drive.unc.out)
traceplot(jags.drive.unc.out)


```



# Adding Random effects:
Now, one can imagine that different drivers might driver different distances at a given speed, leading to different intercepts (and possibly different slopes) for each driver. We will add simulate different intercepts for multiple drivers, to demonstrate how you can add random effects within the JAGS bayesian framework.

```{r}
# driver 1
driver1 <- data.frame(speed = cars$speed + rnorm(50, 10, 2), 
                      dist = cars$dist + rnorm(50, 0, 0.2), 
                      driver = 1)
# driver 2
driver2 <- data.frame(speed = cars$speed + rnorm(50, -7, 2), 
                      dist = cars$dist + rnorm(50, 0, 0.2), 
                      driver = 2)
# driver 3

driver3 <- data.frame(speed = cars$speed , 
                      dist = cars$dist , 
                      driver = 3)

cars.driver <- rbind(driver1, driver2, driver3)
cars.driver$driver <- as.character(cars.driver$driver)


ggplot()+geom_point(data = cars.driver, aes(dist, speed, color = driver))+theme_bw(base_size = 12) +  ylab("Speed")+xlab("Distance")

ggplot(data = cars.driver, aes(dist, speed, color = driver))+geom_point()+stat_smooth(method = "lm")+theme_bw(base_size = 12) +  ylab("Speed")+xlab("Distance")
```

# How to specify a randome intercept in JAGS:
# to do this, we will modify the alpha term

```{r}
a_linear_regression_random_intercept <- "model{

  # Likelihood
  for(i in 1:n){
    y[i]   ~ dnorm(mu[i],inv.var) # data model 
    mu[i] <- alpha[driver[i]] + beta*xvals[i]  # process model for the linear regression
  }

for (i in 1:ndriver) {
    alpha[i] ~ dnorm(mu_alpha, tau_alpha) # Specify random intercepts for each of the drivers, drawn from a distribution of mean  = mu_alpha and precision tau_alpha
}


 # Priors
  beta ~ dnorm(0,0.0001)
  inv.var   ~ dgamma(0.01, 0.01) # Prior for the inverse variance
  sigma     <- 1/sqrt(inv.var) # note that in JAGS the second argument of the normal distribution is 1/sigma^2, or the precision Tau
  
  
# hyperparameters/priors:

 mu_alpha ~ dnorm(0, 0.0001) # hyperparameter mean random alpha
 
 
 
 #sigma_alpha~dunif(0, 100) # SD hyperparameter for random intercepts
 tau_alpha ~ dgamma(0.01, 0.01) # convert sigma_alpha to a precision (what jags expects for a normal distribution)


}"
```

# lets run this model:

```{r}

# we need to specify the data for jags, and include the number of drivers (ndriver), as well as the driver 
cardriver.data <- list(xvals = cars.driver$dist, 
                       y = cars.driver$speed,  
                       driver = cars.driver$driver, 
                       n = length(cars.driver$speed), 
                       ndriver = length(unique(cars.driver$driver)))
```

### Initialize the model
Initializing the model using jags.model. Note that you can (and probably should generate intial values, but here we let jags do the default).
This step sets up the JAGS model, loads the data, and specifies all paramets and priors.

```{r}
jags.int.model   <- jags.model (file = textConnection(a_linear_regression_random_intercept),
                             data = cardriver.data,
                             #inits = inits,
                             n.chains = 3)
```



```{r}


jags.int.out   <- coda.samples (model = jags.int.model,
                            variable.names = c("alpha","beta", "inv.var", "tau_alpha", "mu_alpha"),
                                n.iter = 5000)

summary(jags.int.out)
traceplot(jags.int.out)


```



```{r}
head(jags.int.out)

jags.int.mat <- as.matrix(jags.int.out) 

xpred <- min(cars.driver$dist): max(cars.driver$dist)
plot(cars.driver$dist, cars.driver$speed)
cred.lines <- list()

# loop through to get the lines
for(i in 1000:2000){
 cred.lines[[i]]<-  data.frame(MCMC.step = i,
                              xpreds = xpred, 
                              ypreds.driver1 = jags.int.mat[i,"alpha[1]"] + jags.int.mat[i,"beta"]*xpred,
 ypreds.driver2 = jags.int.mat[i,"alpha[2]"] + jags.int.mat[i,"beta"]*xpred,
 ypreds.driver3 = jags.int.mat[i,"alpha[3]"] + jags.int.mat[i,"beta"]*xpred)
}

cred.intervals <- do.call(rbind, cred.lines)
cred.intervals.m <- reshape2::melt(cred.intervals, id.vars = c("MCMC.step", "xpreds"))

ggplot(cars.driver, aes(dist, speed, color = driver))+geom_point()+theme_bw(base_size = 12) + ylab("Distance")+xlab("Speed")+geom_line(data = cred.intervals.m, aes(x =xpreds, y = value,  group = MCMC.step, color = variable), alpha = 0.5)+facet_wrap(~variable)
```

### More complex models:

The real benefit to going bayes comes (in my opinion) as you start adding complexity to your model structure, particularly if your dataset of interest has heiarchical structure. This allows you parse some of that additive process error into parameter error, random effects, and fixed effects. 

We could keep heiarchically adding complexity, if that is what our system/problem needed. In our example, perhaps Distance vs Speed relationship varied by driver, but also by the car model. Depending on the data stucture, we might add an additional randome effect for the car. But, if all drivers tested out all the cars we could have a nested randome effect, where $\mu_{alpha}$ was drawn from a distribution of with mean $\mu_{car}$ and precision $\tau_{car}$. However, this heiarchical structure should be based in a knowledge of your system and your data. That is why it is best practice to start with a simple model, then build up heiarachical structures, if needed. 

# ----------------------On your own-----------------------------
### Random slopes + intercepts:
Try adding to our random effects model to include both random slopes and intercepts. 



### More Resources on Bayesian modelling
There are alot of great resources on the background of bayesian statistics and heiarchical models, including textbooks and examples of code. My background is in ecology, so much of my suggestions might be biased. 
 - Doing Bayesian Data Analysis Book
 - Bayesian Models Book
 - Introduction to Applied Bayesian Statistics and Estimation for Social Scientists

### More resources on Bayesian modelling in R
There are alot of different ways to implement Bayesian models in R, each have their advantages, disadvantages, and vary in how intuitive they may be to a beginnner. Below is a list of differnt programs with links for more information

-	JAGS
-	BUGS/WINBUGS
- STAN
- NIMBLE



