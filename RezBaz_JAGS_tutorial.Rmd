---
title: "Introduction to Bayesian Statistics in R"
author: "Kelly Heilman"
date: "5/18/2021"
output:
  pdf_document: default
  html_document: default
---

## Overview 
In this workshop, we will cover the very basics of Bayesian Statistics, and go over an example of how to specify your first Bayesian linear regression in R using JAGS!

## Prerequisites:
If you want to follow along with the code you need to either :
A) 
- install JAGS itself http://mcmc-jags.sourceforge.net/
- have R/Rstudio installed
- have the "rjags", "coda", and "ggplot2" libraries installed 
- clone or download this repository here: https://github.com/Kah5/ResBaz_JAGS_tutorial.git

OR
B) 
- Click on the `launch binder` button from the [repository](https://github.com/Kah5/ResBaz_JAGS_tutorial)
- It may take a minute, but the repository contents and the necessary software and packages will appear a virtual RStudio environment
- Open the .R file to follow along the JAGS model portion of the tutorial



## Objectives
We will cover the very (very) basics of Bayesian statistics and go over an example of how to specify your first Bayesian linear regression in R! At the end of this tutorial, learners should have a better understanding of Bayes theorum, and gain some experience running a Bayesian model in R and JAGS. 

I have also listed further resources and things to watch out for when working in the Bayes frame of mind. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, tidy.opts=list(width.cutoff=80), tidy=TRUE)

library(rjags)
library(coda)
library(ggplot2)
library(ggforce)

```

## What are the advantages of Bayesian analysis?
  - All parameters are/can be treated as as random variables, including our data!
      - rather than assuming our data and covariates are the single *true* estimate, we acknowledge that these values come from a distribution that represents the True (with a capital T) value of the process we are interested in.
  - Allow for estimation of and parsing different sources of uncertainty
      - Bayesian predictive intervals can include different sources of uncertainty: unexplained process error, parameter uncertainty, data uncertainty, and driver uncertainty
      
  - Bayesian methods can accommodate hierarchical and different process models 
      - borrow strength across different groups or treatments
      - can specify appropriate distributions and different process models, including state-space models, logistic functions, non-linear models, etc. 


## Bayes Theorem
Bayes theorem is the basis for Bayesian analyses, so we should introduce it. It states that the conditional probability of A, given B is given by the probability of B given A, multiplied by probability of A, and normalized (divided) by the probability of B.

$$P(A|B) = \frac{P(B|A)*P(A)}{P(B)}$$

Where $P(B|A)$ is the likelihood function, $P(A)$ is the prior or marginal probabilty of A, $P(B)$ is the marginal probability of B.

Now, I am assuming you know a bit about probability, but let's review a minute. Lets review using this Venn diagram. 

# Based on the distribution of points in groups A and B:
1. P(A) Marginal probability of being in group A = (5/12)
2. P(B) Marginal probability of being in group B = (9/12)
3. P(A, B) Joint probability of being in A and B = (2/12)
4. P(A|B) Conditional probability that we are in A, given that we are in B? (2/9)
5. P(B|A) Conditional probability that we are in B, given that we are in A? (2/5)


```{r echo = FALSE, fig.width = 4, fig.height = 2, fig.align='center'}

df.venn <- data.frame(x = c(0.866, -0.866),
                      y = c( -0.5, -0.5),
                      labels = c('B', 'A'))
                      
df.pt <- data.frame(x= c(-1.3, -1, 1, 
                          -0.4, -0.3, -0.7, 
                          1.4, 1.3, 1.1, 1.6, 1.8, 1.2), 
                    y = c(0.43, -1.3, 0.2, 
                          0, -0.4, 0.3, 
                          -0.21, -1.56, -0.75, 0.75, 0.56, 0.2), 
                        name = 1)
                          
venn <- ggplot() +
    geom_circle(data = df.venn, aes(x0 = x, y0 = y, r = 1.5, fill = labels), alpha = .3, size = 1, colour = 'grey') +
      coord_fixed() +  theme_void() 
  venn + geom_point(data = df.pt, aes(x = x, y = y), color = "red")
```
### So what does Bayes theorum mean in the context of a model?

Let's say we have a regression model with parameters denoted by $\theta$ and data denoted by $X$ we can use Bayes theorem to determine the conditional probability of our model parameters, given the data $P(\theta|X)$, also called the *posterior*. 

$$P(\theta|X) = \frac{P(X|\theta)*P(\theta)}{P(X)}$$

The posterior is equal to the the likelihood $P(X|\theta)$ (you might recognize this from frequentist/maximum likelihood methods) times the prior probability of the model parameters $P(\theta)$ 

Because the marginal probability $P(X)$ is the probability of X, determined by the integral of the joint probability $P(X,\theta)$ over all possible states of $\theta$, we can rewrite this as:

$$P(\theta|X) = \frac{P(X|\theta)*P(\theta)}{\int P(X|\theta)*P(\theta)}$$
Note that now the denominator serves to normalize the function. 

### Sun exploding example 

Example From Ecological Forecasting, by Mike Dietze, inspired by comic below XKCDhttps://xkcd.com/1132/:


```{r fig.width=4, fig.height=5,fig.align='center',echo=FALSE}
library(png)
library(grid)
img <- readPNG("frequentists_vs_bayesians.png")
 grid.raster(img)
```

A neutrino detector tells us whether the sun has exploded or not, but has a 1/36 chance of lying to us about the sun going nova.
Our data (X) says "Yes, the sun has gone nova". X = Yes, and we have two hypotheses ($\theta_{1}$ and $\theta_{2}$).

$$\theta_{1} = nova sun$$
$$\theta_{2} = no nova$$, 

So let's derive 2 likelihoods:
$P(X = yes|\theta_{1} = nova) = 35/36$ -- the probability that a yes from the detector is not a lie.

$P(X = yes|\theta_{1} = not nova) = 1/36$ -- the probability that a yes from the detector is actually a lie

So to estimate the posterior probabilities of the sun exploding $\theta_{1}$ = nova, given our the neutrino detector telling us "yes" $X = Yes$:

$$P(\theta_{1} = nova|X= YES) = \frac{P(X = YES|\theta_{1} = nova)*P(\theta_{1}= nova)}{P(X = YES|\theta_{1}= nova)*P(\theta_{1}= nova)+ P(X = YES|\theta_{2}= not nova)*P(\theta_{2}= not nova)}$$

Lets assume that $P(\theta_{1}) = 1/10000$, or a low chance of the sun actually going nova, we can calculate the posterior probability of the sun going nova!

You can test out a few different priors (change prior_theta1 in the code below) to see how our prior belief that the sun has actually exploded affects our posterior estimates. 
```{r}
likelihood.theta1 = 35/36 # probability of detector giving a yes, given the sun has actually exploded
likelihood.theta2 = 1/36 # probability of a detector giving a yes, when the sun has not exploded

prior_theta1 = 1/100000 # prior belief that the sun has gone nova
prior_theta2 = 1- prior_theta1

# plug in values for Bayes theorem:
posterior = (likelihood.theta1*prior_theta1)/
  ((likelihood.theta1*prior_theta1)+(likelihood.theta2*prior_theta2))

# convert to a percentage & round
print (paste("Bayesian: There is a ", round(posterior*100, 5), "% chance the sun just went nova!"))

```
Note that if we have a stronger prior belief that the sun exploded, then the prior has a stronger influence on the posterior. 

In many (perhaps most) cases, practitioners will keep priors "non-informative," and rely mostly on the data. However, there might be some good reasons to use "informative" or strong priors in some cases. For example, you may have previous research or a posterior from a previous analysis that strongly suggests a high probability of the sun going nova. A meta analysis might provide prior information about what the values of certain model parameters should be. The key here is to not double dip--you can't use your data from the analysis to inform the prior.

### Using Bayes theorem to estimate model parameters:
Lets say we have a normal Gaussian likelihood with a mean = $\mu$ SD = $\sigma$, and that we observed a value of X = 15. We could use Bayes theorem to determine the posterior probability of our parameter of interest $\mu$, given our data X = 15: 

The posterior probability of $\mu$ given the data X = 15 is proportional to the likelihood (probability of the data, given the model) multiplied by the prior probability of $\mu$.  We drop the normalizing constant, so the formula is 'proportional to' rather than 'equal to'. 

Assume that our prior probability of $\mu$ is represented by a normal distribution with a mean of $\mu_{0}$ and variance $\tau$. Substituting these distributions gives us the posterior. If you were to analytically solve this (substituting the analytical form of the normal distribution), you would see that that this results in a normal distribution for our posterior, with a posterior mean = a weighted average of the data (X), and the prior mean ($\mu_{0}$), and a posterior precision ($1/\sigma^2$) is the sum of the prior precision and the data precision. 


$$P(\mu|X)  \propto  P(X|\mu)*P(\mu) = Normal(X|\mu, \sigma)*Normal(\mu|mu_{0}, \tau)$$
For those who are visual learners, the below code simulates the prior distribution (red), the likelihood (green), and the posterior (blue). 
```{r, echo = FALSE}

df.example = data.frame(prior = rnorm(10000, 0, sd = 10), 
                        likelihood = rnorm(10000, 15, sd = 2), 
                                           posterior= rnorm(10000, 13, sd = 1.2))
df.example <- reshape2::melt(df.example)
ggplot(df.example, aes(value, color = variable, fill = variable))+geom_density(alpha = 0.5)+theme_bw()+geom_vline(aes(xintercept = 15), linetype = "dashed", color = "red")
```

TLDR: The posterior mean is a weighted average of the prior and data, while posterior precision = sum of data precision and prior precision. 

Question to think about:
-What does this mean for the precision or uncertainty in the posterior, is it more precise, or less precise than the prior?



### Monte Carlo Methods:
In the two examples with Bayes theorem above, we used examples where you could theoretically express the posterior using a commonly used distribution (normal). However, in more complex models, this is not always (not often) the case. However, we can rely on Monte Carlo Methods to approximate...

Numerical methods allow for approximation of any probability distribution using random samples from that distribution, even if you can't write out the analytical form of that distribution. We won't get too much into the weeds about this here, as I want to focus on implementation in R. 

Just know that here we will use Markov chain Monte Carlo (MCMC) methods to randomly sample our posterior parameter distributions. JAGS is one program that uses MCMC methods, but there are several, including:

  -	JAGS/rjags: https://cran.r-project.org/web/packages/rjags/index.html
  -	BUGS/WINBUGS: https://www.openbugs.net/w/FrontPage
  -	STAN: https://mc-stan.org/users/interfaces/rstan
  -	NIMBLE: https://r-nimble.org/

### A simple linear regression example
We will start off thinking about a simple linear regression approach, where we are estimating the slope and intercept of the relationship between two variables in a sample relationship. You might use `lm()` to fit this model using maximum likelihood, frequentist approach. We will use the cars dataset loaded into R.

Frequentist approaches are often based on maximum likelihood methods, that is maximizing the probability of the data given the specified model parameters.


```{r, collapse=TRUE }

# do a basic linear regression using "lm"

speed.dist.lm <- lm(speed ~ dist, data = cars)
lm.results <- summary(speed.dist.lm)

summary(speed.dist.lm)

# I am a big GGPLOT fan, so sorry base R plotters...
ggplot(cars, aes(dist, speed))+
  geom_point()+theme_bw(base_size = 12) + ylab("Distance")+xlab("Speed")+
  geom_abline(intercept = lm.results$coefficients[1,1],
              slope = lm.results$coefficients[2,1], color = "red")

```

### A simple linear regression in JAGS

I mentioned that in Bayesian models, all variables are treated as random, that means that all variables are/can be estimated within the model. In our linear regression, this includes our dependent variable, y, and could include our independent variable x. 

Why might this be useful?
  - Well y is a measurement taken by me, an imperfect human being. 
  - We typically can't directly measure the "TRUE" process or value that we are interested in.
  - We might be estimating a single process/value from two different data sources.


##### The Linear Regression Model

#### Data Model
$$y_{i}~\sim normal(\mu, \sigma^2)$$

#### Process Model 
We specify the linear regression with slope $\beta$, intercept $\alpha$ and our independent variable $x$. 

$$\mu = \beta*x + \alpha$$

While we are specifying a very simple model, the process model could be any model that you can write out (logistic growth, multiple linear regression, state-space model, etc). 


### Priors:
In Bayesian models, you must specify distributions that you want to sample the parameters from. We won't go into detail on how to select priors in this short workshop, but keep in mind that prior selection is quite important. The benefit of Bayesian modeling is that you can choose "uninformative priors" or "informative priors". You typically want to specify 'conjugate priors,' which means that when the prior and the likelihood are combined, they generate a posterior distribution that has a named & defined distribution, such as a normal distribution, Poisson, etc. One common example of conjugacy: for a normal likelihood, normal priors for the mean and gamma priors for the precision. We won't be too concerned about this for this tutorial, but keep this in mind when you are building your own models.

#### Uninformative priors:

Specify a very broad distribution over which to sample for our parameter, and basically state that we have no prior knowledge of what value our parameter should be, besides the distribution shape. 

#### Informative priors:

Specify that we have *some* prior knowledge of what value the parameter should be. 

Why might this be useful?

Well, science does not exist in a vacuum, and we likely already have some prior knowledge of what we expect. For example, this might be useful if we have already done a study where we quantified the relationship between y and x, and we know that the value of beta should fall within a given range. In the XKCD comic sun example we specified an informative prior based on the fact that the sun exploding is fairly unlikely. 

This may not be appear to be super useful in a basic linear regression, it can also be useful in much more complex models, where multiple parameters could trade off. Informative priors can improve model convergence by constraining one or more parameters. 

In this model, we will specify uninformative priors for our parameters, assuming that the true value of beta is sampled from a normal distribution with mean of 0 and variance of 1000. We also specify a prior for $\sigma^2$. 

$$\beta \sim normal(0, 0.001)$$
$$\alpha \sim normal(0, 0.001)$$
$$\sigma^2 \sim invgamma(0.01, 0.01)$$

Note that variances must be non-negative, hence we cannot use a normal prior as a distribution. The domain of the inverse gamma distribution is positive. 

## JAGS (Just Another Gibbs Sampler)
We will use JAGS ('Just Another Gibbs Sampler') to run this particular Bayesian model. JAGS is designed to run Bayesian models using Markov chain Monte Carlo simulations (MCMC). JAGS is just one option among many for Bayesian analysis. We can call and run jags from R utilizing the 'rjags' package. Note that there are other R packages that can do this, including 'R2jags'. 


## Defining our JAGS model
To specify a JAGS model, you must define all the random distributions. 

```{r}
asimple_linear_regression <- "model{

  # Likelihood
  for(i in 1:n){
  
    # data model 
    y[i]   ~ dnorm(mu[i], tau) # data y follows a normal distribution with mean mu and precision tau
    
    # process model for the linear regression
    mu[i] <- alpha + beta*xvals[i]  
  }

 # Priors
  beta ~ dnorm(0,0.0001) # uninformative priors for beta and alpha
  alpha ~ dnorm(0,0.0001)

  
  tau   ~ dgamma(0.01, 0.01) # Prior for the inverse variance, the precision tau
  sigma     <- 1/sqrt(tau) 
  
  # note that in JAGS the second argument of the normal 
  # distribution is 1/sigma^2, or the precision Tau

}"
```


### Formatting the data + inital values for each chain
Jags expects all data to in a list.
```{r}
car.data <- list(xvals = cars$dist, y = cars$speed, n = length(cars$speed))
```

### Initialize the model
Initializing the model using jags.model. Note that you can (and probably should generate initial values, but here we let jags do the default).
This step sets up the JAGS model, loads the data, and specifies all parameters and priors.
```{r}
jags.model   <- jags.model (file = textConnection(asimple_linear_regression),
                             data = car.data,
                             #inits = inits, # you can also specific initial conditions
                             n.chains = 3)
```

### Running MCMC chains 
JAGS can run multiple MCMC chains, ideally with random starting values. This is a good idea because it ensures that our model "converges" on the same parameter space consistently. Many times people run at least 3 chains.

The function `coda.samples` runs the MCMC chains of our model for a specified number of MCMC iterations (specified by 'n.iter'), and is set up to 'trace' or output all parameters/variables of interest specified by variable.names. Often models are run for a high number of iterations, so 5000 MCMC samples is quite small, but we will use it for this tutorial. 

```{r}
jags.reg.out   <- coda.samples (model = jags.model,
                            variable.names = c("alpha","beta", "tau"),
                                n.iter = 5000)

summary(jags.reg.out)
```

### Assessing Model output 
coda.samples gives us a `mcmc.list` 

```{r,  collapse=TRUE }
class(jags.reg.out)
```

### Have our chains converged?
We need to make sure that our random samples are converging to the same parameter space. Tools to do this include:
 -traceplots
 -GBR statistics (ideally < 1.01)
 -remove "burn-in"
 

```{r}
par(mfrow = c(1,3))
traceplot(jags.reg.out)
gelman.diag(jags.reg.out)
```
### apply a burn in period:
Note that in the above traceplots, the first few samples have not quite converged, so we often run the model for a "burn in" period to remove these mcmc samples and only work with the samples that have converged. You can do it when we specify the model, or you can just remove these after the fact. We are removing them after the fact in the code below


### Check for Autocorrelation
# Many people thin their chains by taking only the "ith" MCMC iteration 
```{r}
acfplot(jags.reg.out)
```
### Lets plot up our parameter estimates + uncertainty

# plot the credible intervals around the regression line, which are analogous to the uncertainty/error provided in a frequentist regression 
```{r}

#head(jags.reg.out)

jags.mat <- as.matrix(jags.reg.out) 

xpred <- min(cars$dist): max(cars$dist)
#plot(cars$dist, cars$speed)
cred.lines <- list()

# loop through to get the lines
for(i in 1000:2000){
 cred.lines[[i]]<-  data.frame(MCMC.step = i,
   xpreds = xpred, 
                              ypreds = jags.mat[i,"alpha"] + jags.mat[i,"beta"]*xpred)
}

cred.intervals <- do.call(rbind, cred.lines)


ggplot(cars, aes(dist, speed))+geom_point()+theme_bw(base_size = 12) +
  ylab("Distance")+xlab("Speed")+
  geom_line(data = cred.intervals, aes(x =xpreds, y = ypreds, group = MCMC.step),
            color = "grey", alpha = 0.5)


```

# Lets also simulate the credible intervals and predictive intervals from random pairs of samples of the mcmc output
```{r}
npred <-length(min(cars$dist): max(cars$dist))
nsamp <- 1000
mcmcsamples <- sample(nrow(jags.mat), nsamp)
xpred <- min(cars$dist): max(cars$dist) 	
ypred <- matrix(NA ,nrow=nsamp, ncol=npred)	
ycred <- matrix(NA, nrow=nsamp, ncol=npred)	
```

Next we'll set up a loop where we'll calculate the expected value of y at each x for each pair of regression parameters and then add additional random error from the data model to produce the posterior predictive distribution. 

```{r}
for(i in 1:1000){
  params <-  jags.mat[mcmcsamples[i],]
  ycred[i,] <- params["alpha"] + params["beta"]*xpred # same as above
  ypred[i,] <- rnorm(n = npred, mean = ycred[i,], sd = 1/sqrt(params["tau"])) #draw from normal distribution with our estimated uncertainty!
}


#combine the  ci, pi, and xpreds into a dataframe to plot in ggplot2

ci.and.pi <- data.frame(xvals = xpred, 
                       ci.med = apply(ycred, 2, quantile, 0.5),
                       ci.lo= apply(ycred, 2, quantile, 0.025),
                       ci.hi= apply(ycred, 2, quantile, 0.975),
                       pi.med= apply(ypred, 2, quantile, 0.5),
                       pi.lo= apply(ypred, 2, quantile, 0.025),
                       pi.hi=apply(ypred, 2, quantile, 0.975))  

# now make a pretty plot with all the 95% credible intervals and 95% posterior predictive intervals on it!

ggplot()+geom_point(data = cars, aes(dist, speed))+theme_bw(base_size = 12) + 
  
  ylab("Speed")+xlab("Distance")+
  geom_ribbon(data = ci.and.pi, aes(x = xvals, ymin = pi.lo, ymax = pi.hi), fill = "blue", alpha = 0.6)+
  geom_ribbon(data = ci.and.pi, aes(x = xvals, ymin = ci.lo, ymax = ci.hi), fill = "grey", alpha = 0.6)

```


## Breakout group Discussion

#### In your groups, discuss:
  
  1. What do the credible intervals represent? 
  2. What aspects of uncertainty do you think the credible intervals include?
  3. What aspects of uncertainty do the posterior predictive intervals include?



## Sources of uncertainty that Bayesian modelling can include:

  1. parameter uncertainty (credible intervals, the 95% CI around $\beta$ and $\alpha$)
  2. process uncertainty (sigma)
  3. driver/data uncertainty (What if we had multiple measurements of our dependent variable?)
  4. random effects uncertainty/parameter variability


### Including Data Uncertainty or "Uncertainty in variables" 

Lets say that our variable "Speed" was measured by taking 3 different radar measurements, each slightly different. How do we know what the "true" speed is?

Open question for the group: How could you typically deal with this?


What if we could include this uncertainty in our model, in a way that propagates forward to the estimates of distance?

We can (in a few different ways)!

One way to do this is to add a data model for speed:
Lets say we took multiple measurements of speed for several distances, and determined that our speed estimates have a SD = 2.5. 

We can use this measurement to estimate the "true speed" in our model:
$$speed_{i} \sim Normal(mean = truespeed_{i}, precison = tau_{speed})$$
Since we already have some prior knowledge of what the precision on our speed estimates should be, we can include an informative prior:

$$tau_{speed} \sim gamma(10, 0.5)$$

```{r}
a_linear_regression_driver_unc <- "model{

  # Likelihood
  for(i in 1:n){
    
    # data model for speed measurements
   true_speed[i] ~ dnorm(xvals[i], tau_speed) 
   
    # data model for distance (y variable)
    y[i]   ~ dnorm(mu[i],inv.var) 
    
    # process model for the linear regression
    mu[i] <- alpha + beta*true_speed[i]  
  }

 # Priors
  beta ~ dnorm(0,0.0001)
  alpha ~ dnorm(0,0.0001)

  
  inv.var   ~ dgamma(0.01, 0.01) # Prior for the inverse variance
  sigma     <- 1/sqrt(inv.var) # note that in JAGS the second argument of the normal distribution is 1/sigma^2, or the precision Tau
  
  # prior for speed precsion
  tau_speed   ~ dgamma(10, 0.5) 

}"
```


```{r}
car.data <- list(xvals = cars$dist, y = cars$speed, n = length(cars$speed))
```

```{r}
jags.model.drive   <- jags.model (file = textConnection(a_linear_regression_driver_unc ),
                             data = car.data,
                             #inits = inits,
                             n.chains = 3)
jags.drive.unc.out   <- coda.samples (model = jags.model.drive,
                            variable.names = c("alpha","beta", "inv.var", "tau_speed" ),
                                n.iter = 5000)

summary(jags.drive.unc.out)
traceplot(jags.drive.unc.out)
```
Adding an informative prior is one way to include the data or driver uncertainty in the model. Another might be to explicitly include multiple measurements/observers in your model.


### Adding Random effects to our JAGS model:
Now, one can imagine that different drivers might drive different distances at a given speed, leading to varying intercepts (and possibly varying slopes) for each driver. We will simulate different intercepts for multiple drivers, to demonstrate how you can add random effects within the JAGS Bayesian framework.

```{r}
# driver 1
driver1 <- data.frame(speed = cars$speed + rnorm(50, 10, 2), 
                      dist = cars$dist + rnorm(50, 0, 0.2), 
                      driver = 1)
# driver 2
driver2 <- data.frame(speed = cars$speed + rnorm(50, -7, 2), 
                      dist = cars$dist + rnorm(50, 0, 0.2), 
                      driver = 2)
# driver 3

driver3 <- data.frame(speed = cars$speed , 
                      dist = cars$dist , 
                      driver = 3)

cars.driver <- rbind(driver1, driver2, driver3)
cars.driver$driver <- as.character(cars.driver$driver)


# ggplot()+geom_point(data = cars.driver, aes(dist, speed, color = driver))+theme_bw(base_size = 12) +  ylab("Speed")+xlab("Distance")

ggplot(data = cars.driver, aes(dist, speed, color = driver))+geom_point()+stat_smooth(method = "lm")+theme_bw(base_size = 12) +  ylab("Speed")+xlab("Distance")
```

# How to specify a random intercept in JAGS:
# to do this, we will modify the alpha term

```{r}
a_linear_regression_random_intercept <- "model{

  # Likelihood
  for(i in 1:n){
   # data model for speed measurements
   
   true_speed[i] ~ dnorm(xvals[i], tau_speed) 
    y[i]   ~ dnorm(mu[i],inv.var) # data model 
    mu[i] <- alpha[driver[i]] + beta*true_speed[i]  # process model for the linear regression
  }

for (i in 1:ndriver) {
    alpha[i] ~ dnorm(mu_alpha, tau_alpha) # Specify random intercepts for each of the drivers, drawn from a distribution of mean  = mu_alpha and precision tau_alpha
}


 # Priors
  beta ~ dnorm(0,0.0001)
  inv.var   ~ dgamma(0.01, 0.01) # Prior for the inverse variance
  sigma     <- 1/sqrt(inv.var) # note that in JAGS the second argument of the normal distribution is 1/sigma^2, or the precision Tau
  
  
# hyperparameters/priors:

 mu_alpha ~ dnorm(0, 0.0001) # hyperparameter mean random alpha
 
 
 
 #sigma_alpha~dunif(0, 100) # SD hyperparameter for random intercepts
 tau_alpha ~ dgamma(0.01, 0.01) # convert sigma_alpha to a precision (what jags expects for a normal distribution)
# prior for speed precsion
  tau_speed   ~ dgamma(10, 0.5) 

}"
```

# lets run this model:

```{r}

# we need to specify the data for jags, and include the number of drivers (ndriver), as well as the driver 
cardriver.data <- list(xvals = cars.driver$dist, 
                       y = cars.driver$speed,  
                       driver = cars.driver$driver, 
                       n = length(cars.driver$speed), 
                       ndriver = length(unique(cars.driver$driver)))
```

```{r}
jags.int.model   <- jags.model (file = textConnection(a_linear_regression_random_intercept),
                             data = cardriver.data,
                             #inits = inits,
                             n.chains = 3)
```

```{r}

par(mfrow = c(2,4))


jags.int.out   <- coda.samples (model = jags.int.model,
                            variable.names = c("alpha","beta", "inv.var", "tau_alpha"),
                                n.iter = 5000)

#summary(jags.int.out)
traceplot(jags.int.out)


```


# plot the credible intervals for each 
```{r}
#head(jags.int.out)

jags.int.mat <- as.matrix(jags.int.out) 
 jags.int.mat <- jags.int.mat[1000:15000,]#select burn in of 1000
xpred <- min(cars.driver$dist): max(cars.driver$dist)
plot(cars.driver$dist, cars.driver$speed)
cred.lines <- list()

# loop through to get the lines
for(i in 1000:2000){
 cred.lines[[i]]<-  data.frame(MCMC.step = i,
                              xpreds = xpred, 
                              ypreds.driver1 = jags.int.mat[i,"alpha[1]"] + jags.int.mat[i,"beta"]*xpred,
 ypreds.driver2 = jags.int.mat[i,"alpha[2]"] + jags.int.mat[i,"beta"]*xpred,
 ypreds.driver3 = jags.int.mat[i,"alpha[3]"] + jags.int.mat[i,"beta"]*xpred)
}

cred.intervals <- do.call(rbind, cred.lines)
cred.intervals.m <- reshape2::melt(cred.intervals, id.vars = c("MCMC.step", "xpreds"))
cred.intervals.m$driver <- ifelse(cred.intervals.m$variable %in% "ypreds.driver1", "1", 
                                  ifelse(cred.intervals.m$variable %in% "ypreds.driver2", "2", "3"))


ggplot(cars.driver, aes(dist, speed, color = driver))+geom_point()+theme_bw(base_size = 12) + ylab("Distance")+xlab("Speed")+geom_line(data = cred.intervals.m, aes(x =xpreds, y = value,  group = MCMC.step, color = driver), alpha = 0.5)+facet_wrap(~variable)
```
### Uncertainty in the model:

## So why Bayes?

The real benefit to going Bayesian comes (in my opinion) as you start adding complexity to your model structure, particularly if your dataset of interest has hierarchical structure. This allows you partition some of that additive process error into parameter error, random effects, and fixed effects. 

We could keep hierarchically adding complexity, if that is what our system/problem needed. In our example, perhaps Distance vs Speed relationship varied by driver, but also by the car model. Depending on the data structure, we might add an additional random effect for the car. But, if all drivers tested out all the cars we could have a nested random effect, where $\mu_{alpha}$ was drawn from a distribution of with mean $\mu_{car}$ and precision $\tau_{car}$. However, this hierarchical structure should be based in a knowledge of your system and your data. That is why it is best practice to start with a simple model, then build up hierarchical structures, if needed. 

Bayesian approaches can also be quite useful when it comes to partitioning uncertainty, and making forecasts with uncertainty. For example, in our above model, we have estimates of how much uncertainty is due to parameters (beta, alpha), random effects, process error (sigma), data uncertainty (tau_speed). Bayesian methods allow lend themselves to evaluating these uncertainties and propagating uncertainties into predictions/forecasts. 


## On your own:
### Random slopes + intercepts:
Try adding to our random effects model to include both random slopes and intercepts. 


### More Resources on Bayesian modelling
There are many great resources on the background of Bayesian statistics and hierarchical models, including textbooks and examples of code. My background is in ecology, so much of my suggestions might be biased. 

 - Doing Bayesian Data Analysis: A tutorial with R, JAGS, and STAN, John Kruschke https://sites.google.com/site/doingbayesiandataanalysis/
 - Bayesian Models:  A Statistical Primer for Ecologists, Hobbs N. T., & Hooten, M. B. (2015). Princeton University Press.
 - Introduction to Applied Bayesian Statistics and Estimation for Social Scientists, Scott M. Lynch  (2007). Springer-Verlag. https://doi.org/10.1007/978-0-387-71265-9
 - Ecological Forecasting, Michael Dietze. (2017). https://press.princeton.edu/books/hardcover/9780691160573/ecological-forecasting


### More resources on Bayesian modelling in R
There are also different ways to implement Bayesian models in R, each have their advantages, disadvantages, and vary in how intuitive they may be to a beginner. Below is a list of different programs with links for more information:

-	JAGS: http://mcmc-jags.sourceforge.net
-	BUGS/WINBUGS: https://www.mrc-bsu.cam.ac.uk/software/bugs/
- STAN: https://mc-stan.org/users/interfaces/rstan
- NIMBLE: https://r-nimble.org

